---
layout: post
title: "sas notes"
description: ""
category: 
tags: [sas]
---
{% include JB/setup %}

# NLMIXED #

## METHOD ##

1. DEFAULT: [GAUSS] adaptive Gauss-Hermite Quadrature
2. [FIRO] : First-Order Method : only for **Normal** outcome and **Random** is used
3. [HARDY] : Hardy Quadrature : only **ONE** Random
4. [ISAMP]: Adaptive Importance Sampling: additional option: (**seed**)

For *GAUSS* and *ISAMP*, NO ADAPTIVE QUADRATURE and NO ADAPTIVE SCALING can also be used (**noad** and **noadscale**)

## TROUBLESHOOT ##

1. sort by subject
2. no misspell in `PARAM`
3. no same name in data set
4. reparamatrize : `exp(2\*logsd_u)`
5. rescale
6. `GCONV = 0` makes `FCONV`; change to `GCONV = 0.01` might help
7. look at convergence : change covnrgence criteria
8. try simpler model first 
9. `PARMS` for starting point
10. try previous different convergence methods

# SORT #

	PROC SORT DATA = INPUT OUT = OUTPUT [NODUPLICATES];
		BY [DESCENDING] VAR [VAR2] [_ALL_];
	RUN;

# UNIVARIATE #

## HISTOGRAM ##

	proc univariate data=Steel;
		var Length Width;
		histogram;
		ods output moments  = a;
	run;

# MEANS #

	PROC MEANS DATA = KISD NWAY;
		CLASS GROUP;
		VAR age wt;
		OUTPUT OUT = outdata MEAN = aveage avgwt;
	RUN;

sort in advance

# GPLOT #

	symbol1 I=R V=circle;

	PROC GPLOT DATA=a;
		plot avgfall*time avginj*time;
	RUN;

# FREQ #

	PROC FREQ DATA=BABA NLEVELS;
		TABLES idunit;
	RUN;

# SURVEYSELECT #

	PROC SURVEYSELECT METHOD = SRS (URS) SAMPLESIZE = 10 REP = 1 SEED = 110 OUT = datout;
		id _all_ ;
	RUN;
	
or use `samprate = 0.4`

# NLMIXED vs GLIMMIX #

## Syntax ##

	proc glimmix data=fake;
    class treat ident;
	model y/m = treat / ddfm=residual;
	random ident;
	parms (0.25) (1) / hold=2;
    error=binomial, link=logit;
	run;

	proc nlmixed data=fake tech=trureg df=297;
    bounds sigma2>0;
    parms mu=1 t1=-2 t2=2 sigma2=0.25;
    if treat=1 then eta=mu + t1 + e;
    else if treat=2 then eta=mu + t2 + e;
    else eta=mu + e;
    prob=exp(eta)/(1+exp(eta));
    model y ~ binomial(m, prob);
    random e ~ normal(0, sigma2) subject=ident;
    contrast 'treat' t1, t2;
    predict prob out=results;
	run;

## Pros ##

- Likelihood can be user-specified if distribution is non-standard (**ZI-NB**)
- More exact than glimmix or nlinmix and runs faster than both of them. Some say slower. 
- GLIMMIX: pseudo-likelihood  vs NLMIXED: true log likelihood MLE (approximate) by Adaptive Gauss-Hermite quadrature. This estimation can also be available in GLIMMIX (method = QUAD in GLIMMIX), which can do nested model comparison
- GLIMMIX dependent variables have to be from an exponential distribution
- For marginal log likelihood approximation: NLMIXED more accurate (integral approximation by Gaussian quadrature), where GLIMMIX by linearization, potentially biased estimates for both fixed effects and covariane parameters, especially for binary data. 
- GLIMMIX wald tye test and confidence intervals and nested models can not be compared with true likelihood ratio test
- INITIAL VALUE: NLMIXED > GLIMMIX; For GLIMMIX, more sensitive to initial values


## Cons ##

- Random effects must come from a (multivariate) normal distribution
- All random effects must share the same subject (i.e. cannot have multi-level mixed models)
- Random effects cannot be nested or crossed
- DF for Wald-tests or contrasts generally require manual intervention
- Statistically, GLIMMIX can handle more random effects than NLMIXED. 
- GLIMMIX allow REML , better for number of higher level units is small. NLMIXED no REML

## Detail ##

There are some problems which GLIMMIX can handle with much
greater facility than NLMIXED (e.g., models with both
random effects and repeated measurements).  On the other
hand, the NLMIXED procedure may handle some problems which
the GLIMMIX procedure cannot handle very well.  For instance,
you might have a study which is subject to random effects
and your response follows a zero-inflated negative binomial
distribution.  The GLIMMIX procedure would not be able to
fit a zero-inflated negative binomial response, but the
NLMIXED procedure could be employed.

In general, the NLMIXED procedure is considerably more
computationally intensive than is the GLIMMIX procedure,
even though the GLIMMIX procedure is doubly iterative.
Seldom will the NLMIXED procedure require less computational
resources than GLIMMIX for equivalent problems.

The GLIMMIX procedure also fits mixed models for nonnormal data with nonlinearity in the conditional mean function. In contrast to the NLMIXED procedure, PROC GLIMMIX assumes that the model contains a linear predictor that links covariates to the conditional mean of the response. The NLMIXED procedure is designed to handle general conditional mean functions, whether they contain a linear component or not. As mentioned earlier, the GLIMMIX procedure by default estimates parameters in generalized linear mixed models by pseudo-likelihood techniques, whereas PROC NLMIXED by default performs maximum likelihood estimation by adaptive Gauss-Hermite quadrature. This estimation method is also available with the GLIMMIX procedure (METHOD=QUAD in the PROC GLIMMIX statement).  


## Summary ##

1. NLMIXED: 
   - model is simple and limited to two levels
   - number of random effects is small
   - number of level-2 groups is relatively large
   - binary data require accurate covariance parameter
   - user-defined response distribution
   - nested models need to be compared using the likelihood ratio
  
2. GLIMMIX:
   - more complex models
   - a large number of random effects
   - more than two levels
   - number of higher-level units is small
